%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{graphicx}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\newcommand{\war}[1]{{\sf\small #1}}

\title{WikiTopics: What is popular on Wikipedia and why.}

\author{Byung Gyu Ahn, Chris Callison-Burch, Benjamin Van Durme \\
  Center for Language and Speech Processing \\
  Johns Hopkins University \\
  Baltimore, Maryland \\
  {\tt \{bahn, ccb, vandurme\}@cs.jhu.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
The popularity that social networks such as Twitter and Facebook have recently gained calls more research into the use of metadata that they generate. In this paper, we make use of the page view counts of Wikipedia articles to predict topics gaining in popularity and explain why these topics are gaining users' attention. Our proposed approach clusters related topics to distinguish the main topics of on-going events from secondary related topics and explains the relationship between them. We explain why this is an interesting problem and compare approaches using the link structure of Wikipedia articles and the bag of words model of them to process each step of the process. By combining each step that performs reasonably as well as human annotators, the methods put forth here make a nice summary of what people in the real world are really insterested in.

For sets of articles that show a sharp increase in page view on the same date, we do the following:
(1) cluster articles into subsets that correspond to the same current event, (2) select sentences from the articles that describe why the event is popular.
We evaluate against manually clustered articles and hand selected sentences.
We show the value of bag-of-words topic models, link structure, named entity and time expression recognition.
The result shows promise for explaining what's currently popular on Wikipedia and for creating a timeline of past newsworthy events.

\end{abstract}

\section{Introduction}

Recently, social media such as Twitter and Facebook gained attention from researchers.
By looking at the topics recently prevailing in a lot of Twitter data at the same time, one may know the recent popular topics \cite{petrovic10}.

%Wikipedia has some relative advantage over Twitter as a source of topics, in that (1) it has a full description about the topic, (2) it has inter-article links that imply the relations between articles, and (3) the page view statistics are open to the public.

%The page view statistics are publicized in one of the wikipedians' website. Hour-by-hour statistics of size of 50-80Mbytes, each gzipped. The unzipeed file contains the project name, the article title, the page view counts, and the page size per line.

%\cite{ldc04} describes the definition of topics and events used in the TDT dataset. 

%* System overview describing each module, and including a diagram of how they connect (1.5 pages)
 %- include description of hour-by-hour wikipedia page view stats
   %+ show example graphs that highlight trending topics

%We first discuss previous work related to our work (\S2), then discuss our experimental design (\S3), and move on to show our experimental result (\S4). We then discuss our future work (\S5).

%There is an open-source software called Trending Topics (\url{http://www.trendingtopics.org/}) that shows the recent popular topics extracted from Wikipedia. We employed the same algorithm, which sorts the topics by the difference between the sum of the page counts of the recent 15 days and the sume of the page counts of the previous 15 days. Then, we picked the 100 topics of the highest differences.

In this paper we analyze a novel data set: we have collected the hourly page view statistics for every Wikipedia page in every language for a three year period. We show how these page view statistics--along with a whole host of other features like inter-page links, edit histories, mentions in contemporaneous news stories--can be used to identify and explain popular trends, including political elections, natural disasters, sports championships, popular films and music, and other current events.

Our approach is to select a set of articles whose daily page views increase above their average from the previous two week period. Rather than simply selecting the most popular articles for a given day, this selects articles whose popularity is rapidly increasing. These popularity spikes are presumably due to some external current event in the real world. On any given day, there are many articles whose popularity is spiking.

In this paper we attempt to cluster 100 such articles from each of 5 randomly selected days in 2009, such that the clusters coherently correspond to current events.

We compared our automatically collected clusters to the Wikipedia current events. Wikipedia editors compiles current events every day, which mainly consist of social and political events, traffic accidents and disasters. More often than not, they do not generate much traffic, and link to pages that are too general like ``United States'' or ``Israel''. We view this work as an automatic mechanism that could potentially supplant the hand-curated method of selecting current events that is currently done by Wikipedia editors.

\begin{figure}
\centering
\begin{tabular}{|c|}
\hline
\war{Barack Obama} \\
\war{Joe Biden} \\
\war{White House} \\
\war{Inauguration} \\
\dots \\
\war{US Airways Flight 1549} \\
\war{Chesley Sullenberger} \\
\war{Hudson River} \\
\dots \\
\war{Super Bowl} \\
\war{Arizona Cardinals} \\
\hline
\end{tabular}
\caption{The automatically selected articles for January 27th, 2009. The underscores are used in place of spaces by Wikipedia.}
\label{fig:topics-jan-27}
\end{figure}

For instance, we would attempt to cluster the articles in Figure~\ref{fig:topics-jan-27} into 3 clusters, \{ \war{Barack Obama}, \war{Joe Biden}, \war{White House}, \war{Inauguration} \} which corresponds to the inauguration of Barack Obama, \{ \war{US Airways Flight 1549}, \war{Chesley Sullenburger}, \war{Hudson River} \} which corresponds to the successful ditching of an airplane into the Hudson river without loss of life, and \{ \war{Superbowl}, \war{Arizona Cardinals} \} which describes the then upcoming Superbowl XLIII.

We further try to explain the clusters by selecting sentences from the revision of the Wikipedia articles on that date. For the first cluster, a good selection might be ``the inauguration of Barack Obama as the 44th president of the United States took place on Jan 20, 2009''. For the second cluster, ``Chesley Burnett ``Sully'' Sullenberger III (born January 23, 1951)  is an American commercial airline pilot, \ldots, who successfully carried out the emergency water landing of US Airways Flight 1549 on the Hudson River, offshore from Manhattan, New York City, on January 15, 2009, \ldots.''. For the third cluster, ``[Superbowl XLIII] will feature the American Football Conference champion Pittsburgh Steelers (14-4) and the National Football Conference champion Arizona Cardinals (12-7) .'', which makes clear the relationship with \war{Arizona Cardinals}.

To generate the clusters we can make use of the text of the articles on that date, versions of the articles from previous dates, the link structure and category info from Wikipedia, and potentially external info like newspaper articles published before the date.

To select sentences we may want to make use of NLP technologies such as coref resolution, named entity and date taggers, and dependency parsers to identify subjects of sentences.

\section{Motivation}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/obama_sparkline.png}
\caption{Page views for the articles related to the inauguration of Barack Obama. The articles are linked from an item in the Wikipedia current events. Interestingly, the list does not include the article \war{Inauguration of Barack Obama}, the very page about the event that has a spiking page views.}
\label{fig:obama-sparkline}
\end{figure}

What are interesting topics? In an online encyclopedia such as Wikipedia, the page view counts for each article reflects the popularity of the article. Each article has its own level of normal popularity: some has high page views, and others low. Sometimes, a newsworthy event such as a major political or sports events, or natural disasters, or pandemic, occurs and incur that many articles related to the specific event has a major increase in page views.

The wikipedia has a section called current events, in which the recently occurred events are listed manually by Wikipedia editors. Figure~\ref{fig:obama-sparkline} shows the page views of the articles related to the inauguration of Barack Obama. Each event may have a hierarchical structure--there may be a major event that related to minor events. Each event is described in a line of text with a possibly multiple links to the related Wikipedia articles. The figure shows the spikes in page views of the related articles around the date on which the event took place--January 20th, 2009.

We have selected the articles that has a major increase in page views. The increase in page views are measured as follows. For each day, the total sum of the page views for the past 15 days and the total sum of the page views for the previous 15 days are summed, and the difference $\Delta$ between the two is calculated for each article. The articles are sorted in order of decreasing $\Delta$, and the top 100 articles are selected. The difference $\Delta$ is formularized as below:

\begin{align*}
\Delta = S_{15} - S_{30}, \mbox{ where } S_{15} & = \sum_{i=1}^{15} v_i \mbox{ and }\\
S_{30} & = \sum_{i=16}^{30} v_i
\end{align*}

where $v_i$ is the page views of the article on the past $i$th day from the date on which the articles are selected.

We compared the automatically selected articles to the articles linked from the Wikpedia current events in some aspects. First of all, note that the hand-curated articles are less than half of the automatically selected articles: There are 17,253 hand-selected articles and 36,400\footnote{The year 2009 has 365 days and one day is missing from our daily statistics.} auto-selected articles. Only 28\% of those hand-selected articles are automatically selected. When analyzed, it was found that many of the hand-selected articles have very low page views: 6,294 (36.5\%) have maximum daily page views less than 1000 in 2009. Naturally, they are not chosen by automatic selection based on page views\footnote{The automatically selected articles has an increase in page views of at least 10,000.}.

Figure~\ref{fig:comparison-articles} shows the comparison of the selected articles. Automatically selected articles include an newly created article about a politcal event (\war{Inauguration of Barack Obama}), a recently released film, a popular TV series and related articles and tend to be specific than hand-selected articles. The hand-selected articles include more generic articles related a specific event, most of which are personal, organizational or geopolitical names. The hand-generated event describes the relationships between related articles.

Should we try to predict the current event descriptions that Wikipedia editors hand-curate? We say no for the following reasons. Therefore we recommend against this methodology for other researchers.

We set up a website\footnote{See http://ANONYMIZED} that you can see the sparkline graphs of pageviews for each day, each link, or each event in the form as Figure~\ref{fig:obama-sparkline}. You can see the clear correlation between the spikes of the page views of the articles and the date on which the articles appear as the current events.

At this point, we set the goal of our novel task: to summarize recent events related to popular articles just as the hand-curated Wikipedia current events, but with more popular articles with high page views. This work can be used to replace the hand-generated current events listing the events that many people are interested in.

\begin{figure}
\centering
\begin{tabular}{|c|}
\hline
WikiTopics \\
\hline
\war{Inauguration of Barack Obama} \\
\war{Joe Biden} \\
\war{Notorious (2009 film)} \\
\war{The Notorious B.I.G.} \\
\war{Lost (TV series)} \\
\hline
Wikipedia current events \\
\hline
\war{Fraud} \\
\war{Florida} \\
\war{Hedge fund} \\
\war{Arthur Nadel} \\
\war{Federal Bureau of Investigation} \\
\hline
\end{tabular}
\caption{ The example articles for January 27th. These are the articles that do not have a counterpart with a window size of 15 days. The hand-selected articles are linked from an event ``Florida hedge fund manager Arthur Nadel is arrested by the United States Federal Bureau of Investigation and charged with fraud.'' \\ }
\label{fig:comparison-articles}
\end{figure}

%Some of the most charateristic instances from the topics and the events are shown in Table~\ref{tab:topics-events-comparison}. We evaluated the precision and recall score of the TrendingTopics topics against the Wikepedian Current Events (Table~\ref{tab:topics-events-evaluation}. Their precision and recall scores are extremely low. Probably, wikipedians are more interested in political and scientific topics, and the page view statistics tend to be more dramatic for the societal and cultural events--recent deaths of famous people, recent release of movies and music albums.

%We have selected the articles with more page views above the previous two weeks.
%\cite{petrovic10}
%What are interesting topics? This could be an aesthetic question, but, in Wikipedia, gaining popularity usually means a big inflow of users looking up an article of a specific topic. More often than not, a popular on-going event accompanies many topics, or Wikipedia articles, collecting sudden increases in page view counts (Figure~\ref{fig:topics-for-an-event}). The TrendingTopics algorithm (\S\ref{sec:related-work}) finds a big increase in page view counts and make a list of the articles with the highest increase. 

%Due to the discrepancy between the Wikipedian Current Events and the topics based on page views, it was not realistic to predict from the page view counts the topics that would overlap the Current Events. We set the topics extracted by the TrendingTopics algorithm as the baseline data.

%Figure~\ref{fig:process} outlines the subprocesses discussed in this paper. The first phase, {\it topic selection} is the step that 

%\section{Topic selection}

%\paragraph {Design}
%\begin{figure}
%\begin{picture}(200,100)
%\end{picture}
%\caption{A sparkline graph that shows page views of related topics of an event.}
%\label{fig:topics-for-an-event}
%\end{figure}

%\begin{table}
%\begin{picture}(200,100)
%\end{picture}
%\caption{Evaluation of the TrendingTopics topics against the Current Events}
%\label{tab:topics-events-evaluation}
%\end{table}

Our system pipeline is explained as follows. First, the most popular pages, or topics, are collected per each day(\S2). Second, clusters are identified by clustering related topics and the main article of each cluster is identified(\S3). Lastly, the sentence that best describes the cluster are extracted for each cluster(\S4). See Figure~\ref{fig:process}.

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figures/WikiTopicsPipeline.pdf}
\caption{Process diagram. (a) Topic selection. (b) Clustering. (c) Textualization.}
\label{fig:process}
\end{figure*}

\section{Article selection}
\paragraph{Dataset}
The Wikipedia Traffic Statistics dataset is originally publicized and made available at http://dammit.lt/wikistats/ by a Wikipedian Domas Mituzas.
This data is only kept up to a several months that the space allows.
For the previous statistics, two sets of the statistics are published at Amazon Public Datasets (http://aws.amazon.com/datasets/, http://aws.amazon.com/datasets/2596, http://aws.amazon.com/datasets/4182).
This dataset consists of the files that each has hourly page view statistics for every article in every language.
Each line of the files contains the language or the project name, the title, the hourly page views, and the numbers of bytes of the text of an Wikipedia article.

We limited the work only to the English Wikipedia.
These statistics are collected from the Wikipedia cache server as requested by users, and it includes many wrongful or malicious requests.
Many requested pages are also redirect pages that automatically refer the requester into another page.
The redirect pages are usually the ones that are different names of an entity.
To process these difficulties, we downloaded the English Wikipedia dump on June 22nd, 2010 from Wikimedia dump (http://download.wikipedia.org/) and from the database dump extracted the list of the titles of all articles and the redirect articles.
Using these data, we filtered out the request for non-existing articles and merged the page views for the redirect pages into the main articles.
Also the title of the Wikipedia articles has to be normalized according to a specific format, that is, the first letter of each title are capitalized and a space in it has to be replaced with an underscore, and so on.

\section{Clustering}
\label{ssec:clustering}

More often than not, some of the popular articles are about a current event.
For example, the hand-selected articles shown in Figure~\ref{fig:comparison-articles} are about a current event that ``Florida hedge fund manager Arthur Nadel is arrested \ldots and charged with Fraud.''
Among the automatically selected articles, main events such as \war{Inauguration of Barack Obama} and release of the file \war{Notorious (2009 film)} involves making popular the incidental articles about the players of the events such as \war{Joe Biden} and \war{The Notorious B.I.G.} along with the articles about the main events themselves.

We attempt to cluster the automatically selected articles into mutually related articles and find the article that describes the main event for each cluster. For clustering, we make use of the unigram bag-of-words model and the link structures of articles. To find the centroid articles that describes the main event, we used K-means model and the link structure of articles.

\paragraph{Dataset} For each day of the five selected dates in 2009, we downloaded the text of the 100 automatically selected articles from Wikipedia. The downloaded texts are the latest texts as of the date on which the article is selected. We use the Wikipydia module, which is a python module to make use of the Wikipedia API. As preprocessing, we stripped out all HTML tags from the article text, and replaced the Wikipedia-specific tags as the corresponding text using the mwlib library, and finally split sentences using the NLTK splitter.

\paragraph{Design}
As a baseline, K-means clustering was performed on the set of articles, treating the article texts as bag-of-words. There are 100 automatically selected articles on each of the five selected dates and we set 50 as the number of resulting clusters. We used the Mallet\footnote{Reference?} software to run K-means software. Normalization and tokenization are not performed before running K-means. The algorithm calculates the mean of each cluster in word-vector space, and we chose the centroid article that is closest to the center in the vector space.

We also used the link structures of the Wikipedia articles. The link structures are downloaded from the website of Henry Haselgrove (http://users.on.net/~henry/home/wikipedia.htm).

\paragraph{Evaluation}
Three annotators performed manual clustering on the topics for the five specified dates to get the gold standard clusters.
The three manual clusters were evaluated against each other to measure the annotator agreement,
using the multiplicity B-cubed metric\cite{amigo09} that can handle overlapping clusters.

The B-cubed metric is one of the extrinsic clustering evaluation metrics, which need a gold standard set of clusters to evaluate the set of clusters of interest against.
Each item $e$ has potentially multiple gold standard categories, and also potentially multiple clusters.
Let $C(e)$ is the set of the clusters that $e$ belongs to, and $L(e)$ is the set of $e$'s categories.
The multiplicity B-cubed scores for a pair of $e$ and $e'$ are evaluated as follows:

$$ \mbox{Prec}(e,e') = { \min \left( |C(e) \cap C(e')| , |L(e) \cap L(e')| \right) \over |C(e) \cap C(e')| } $$
$$ \mbox{Recall}(e,e') = { \min \left( |C(e) \cap C(e')| , |L(e) \cap L(e')| \right) \over |L(e) \cap L(e')| } $$

The overall B-cubed scores are evaluated as follows:

$$ \mbox{Prec} = \mbox{Average} _{e \neq e'} \mbox{Prec}(e,e') $$
$$ \mbox{Recall} = \mbox{Average} _{e \neq e'} \mbox{Recall}(e,e') $$

\paragraph{Analysis}

The bag of words model we used does not distinguish different meanings of words. This could result in wrong clustering. For example, the automatically selected articles include both \war{Piracy in Somalia} and \war{The Pirate Bay} as well as \war{Piracy}. Their resemblance in word spelling might result in confusion in clustering.
In the real results, \war{The Pirate Bay} was correctly clustered with \war{The Pirate Bay Trial}, but \war{Piracy} was wrongly clustered with \war{USS Bainbridge (DDG-96)} and \war{MV Maersk Alabama}, both of which are the names of vessels. Instead of \war{Piracy}, \war{Moldova} wrongfully ended up in the same cluster as \war{Somalia} and \war{Piracy in Somalia}.

Trivial as it may sound, but it is not. In automatically selected articles for February 10, 2009, \war{Journey (band)} and \war{Bruce Springsteen} may seem to be relevant to \war{Grammy Awards}, but in fact they are relevant on this day because of the \war{Super Bowl}. The K-means clusters wrongfully clustered the articles relevant to \war{Grammy Awards} and \war{Super Bowl} altogether into one large cluster.

\begin{table}
% table from /Users/bahn/Dropbox/Documents/Research/Wikitopics/Clustering\ Accuracy.xlsx
% CCB is Manual-1, Ben Manual-2, and Byung Manual-3.
\centering
\begin{tabular}{|c|c|c|}
\hline
Gold standard & Evaluated Set & B-Cubed F-score \\
\hline
Manual-1 & Manual-2 & 0.67 $\pm$ {\small 0.076} \\
Manual-1 & Manual-3 & 0.74 $\pm$ {\small 0.085} \\
Manual-2 & Manual-3 & 0.75 $\pm$ {\small 0.129} \\
\hline
Manual-1 & K-means & 0.53 $\pm$ {\small 0.034} \\
Manual-2 & K-means & 0.53 $\pm$ {\small 0.050} \\
Manual-3 & K-means & 0.49 $\pm$ {\small 0.035} \\
\hline
\end{tabular}
\caption{Clustering evaluation. For the B-Cubed metric, exchanging the gold standard and the evaluated dataset incurs the exchange of the precision and the recall score, thus leaving the F-score same.}
\label{tab:clustering}
\end{table}

\section{Textualization}

Textual description of the topics are intended to explain why the topics are popular.
Specifically, the textual description for each cluster may consist of the sentences (1) that describe why the cluster is popular at the time and (2) that describe why each topic in the cluster is popular.
The concepts of central and peripheral topics (\S\ref{ssec:clustering}) are important in that different types of topics contribute to the description in different ways.
Often, the central topics are directly related to the event that caused the recent popularity and often includes a direct explanation of the recent event. 

\paragraph {Preprocess}
We preprocess the Wikipedia articles through the SERIF system\footnote{References?} for the temporal expression identification and the coreference resolution.
The identified temporal expressions are in various formats such as exact date (``February 12, 1809''), a season (``spring''), a month (``December 1808''), a date without a specific year (``November 19''), and even ralative time (``now'', ``later that year'', ``The following year''). Some examples are showed in Figure~\ref{fig:temporal-expressions-examples}.
The coreferences are analyzed into a list of the entities in the article and all the mentions of each entity in the article are compiled as co-ref chains.

\begin{figure}
\centering
\begin{tabular}{|c|c|}
\hline
February 12, 1809 & Later that year \\
1860 & about 18 months of schooling \\
now & November 19 \\
the 17th century & that same month \\
some time & The following winter \\
December 1808 & The following year \\
34 years old & April 1865 \\
spring & late 1863 \\
September & later that year \\
\hline
\end{tabular}
\label{fig:temporal-expressions-examples}
\caption{The temporal expressions identified by the SERIF system in the preprocess step. They are examples selected from 247 such date and time expressions extracted from the article about \war{Abraham Lincoln}}
\end{figure}

\paragraph {Design}
As a baseline, we picked the first sentence for each article because the first sentence generally explains the article.
As a second test set, we picked the sentence with a temporal expression that is most closest on which the article was selected.
Among various date and time forms, the following policy was adapted to define the closeness:
The most specific date with year, month, and day (``February 12, 1809'') has the most precedence over the other ones, and the next is a month with a year (``December 1808''), then a season with a year, and the date without a year.

After selecting a sentence for each cluster, we used coreference resolution to replace the important pronoun in the sentence with its proper name.
This step is important and necessary because the selected sentence often refers to its subject by a pronoun such as ``He'', ``She'', or ``It''.

\paragraph {Evaluation}
For ten articles on a specific day, an annotator selected sentences that best describes why the article is popular from all the sentences in each article.
Each article has about 289 sentences on average.
The annotator picked the best single sentence, and the second best sentences that could potentially be multiple.
In the case there are no best sentence among them, he marked none as the best sentence, and listed all the partially explaning sentence as second best sentences.

The results will be added soon.

\paragraph {Analysis}

We quantitatively analyzed the output of the system, and it suggested
a couple of current problems and future work.

\war{Serena Williams} is an example that the error in sentence splitting
propagates to the sentential selection. The best sentence manually selected
was the first sentence in the article ``Serena Jameka Williams \ldots
, as of February 2, 2009, is ranked World No. 1 by the Women's Tennis
Association \dots .'' The sentence was disastrously divided into two sentences
right after ``No.'' by the NLTK splitter through our preprocess.
It means no matter how well the sentential selection is done,
it cannot choose the gold standard sentence. We ran the splitta
\cite{dgillick09sbd} over the article text and found that it does not
split the first sentence at the wrong position. The better the splitting is,
The better the sentential selection works.

Selection of the best sentence with dates seems to work well,
with some problems.  \war{Farrah Fawcett} is a nice example of multiple sentences
with dates, in a single section, that could potentially be spliced
together into a timeline (the final event, that she was released from
the hospital, makes more sense if we included why she was there).
Furthermore, the sentence describing the most recent event contains
a date without the year, which has less precedence over the other dates with the year
even when it is closer to the date of interest than the others are.
So having precedence over the date forms might not always work well.

An improved baseline for sentence selection would include the opening
sentence of the page from which the date-stamped sentence comes from.
For example, in ``pick0419'' :

\war{Grey Gardens} \# ``It is scheduled to air on HBO on April 18, 2009.''

as compared to:

\war{Grey Gardens} \# Grey Gardens is a 1975 documentary film by the
direction/cinematography/editing team of Albert and David Maysles,
Susan Froemke, Ellen Hovde, and Muffie Meyer. ...  It is scheduled to
air on HBO on April 18, 2009.

From there we'd want to compress the opening sentence, as in this case:

 \war{Grey Gardens} \# Grey Gardens is a 1975 documentary film. ...  It is
 scheduled to air on HBO on April 18, 2009.

 Which then brings up the potential flaw:  it is an *adaptation* of
 this film that is being released on April 18th, not the original.

 Looking deeper, we see the earlier sentence: ``Grey Gardens, a film for
 HBO, starring Jessica Lange and Drew Barrymore as the Edies, with
 Jeanne Tripplehorn as Jacqueline Kennedy, and Daniel Baldwin as Julius
 Krug.''.  This is hard: even if we run a co-ref system over the page to
 resolve the ``It'' in ``It is scheduled to air'', then we'll know that
 ``Grey Gardens'' is scheduled to to air.  But it isn't the same Grey
 Gardens, it is the new adaptation, which we know only from the section
 header.  Or we would have to have background knowledge that could
 enable reasoning about movies not having two different lists of
 starring actors, etc. (even harder!)


\section{Related work}
\label{sec:related-work}

There should be lots of citation to related work here. Examples are NewsBlaster by McKeown and Barzilay; Snippet Selection by Lapata, and a list of ACL/EMNLP papers that study wikipedia.

\section{Future work}

There are still a lot of challanges that remain. For textualization, sentence fusion is an option for summarizing the relevant topics as one event.

\section*{Acknowledgments}

%The first author was supported by Samsung Scholarship.

Thank you to Wikipedians X and Y for making the page view statistics available.

\bibliographystyle{acl}
\bibliography{bahn}
\end{document}
